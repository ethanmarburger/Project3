---
title: "Final Proejct: Modeling File"
format: html
editor: visual
---

```{r}
#| echo: false
#| include: false
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(dplyr)
library(parsnip)
library(tune)
library(ggcorrplot)
library(tree)
library(baguette)
library(ranger)
library(rpart.plot)
library(yardstick)
```

## Introduction

### About the dataset:

The data for this project is from **Diabetes Health Indicators Dataset**. This dataset was put together from the Center of Disease Control's (CDC) Behavior Risk Factor Surveillance System (BRFSS) in 2015. The BRFSS is a premier telephone health-related telephone survey designed to collect state data about United States residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. We are particularly interested in the indicators for Diabetes.

### Transformed variables from the dataset that we'll work with:

**diabetes**: If subject has diabetes or not. While the dataset source mentions a prediabetes level, there are no corresponding values in the dataset.

No Diabetes = no diabetes\
Diabetes = has diabetes

**high_bp**: If subject has high blood pressure or not.

No = no high blood pressure\
Yes = high blood pressure

**high_chol**: If subject has high cholesterol or not.

No = no high cholesterol\
Yes = high cholesterol

**chol_check**: If subject has had their cholesterol checked in the past 5 years.

No = cholesterol not check in past 5 years\
Yes = cholesterol checked in past 5 years

**BMI**: Body Mass Index.

**physical_act**: Physical activity in past 30 days - not including job.

No = no\
Yes = yes

**Sex**: Is the subject male or female.

Female = female\
Male = male

**Age**: Thirteen level category indicating the age of the subject.

Age Ranges\
18-24\
25-29\
30-34\
35-39\
40-44\
45-49\
50-54\
55-59\
60-64\
65-69\
70-74\
75-79\
80 or older

### Purpose of this File:

The goal of this file is to create models for predicting the diabetes variable. We'll be predicting this variable using Classification Tree and Random Forest models. To evaluate the our models, we'll use log-loss as our chosen metric. Additionally, we'll be using 5 fold cross-validation to select the best model from that family of models.

## Predictive Modeling Set up

### Splitting Data into Training and Test Sets

```{r}
# Setting a seed to make things reproducible
set.seed(23)

# tidy models to split the data into a training and test set (70/30 split)
data_split <- initial_split(data, prop = 0.70)
data_train <- training(data_split)
data_test <- testing(data_split)

# 5 fold cross validation on the training set
data_5_fold <- vfold_cv(data_train, 5)
```

### Create our Recipe for Data Preprocessing

```{r}
rec <- recipe(diabetes ~ ., data = data) |>
  step_dummy(high_bp, high_chol, chol_check, physical_act, sex, age) |>
  step_normalize(BMI)
```

## Fitting a tuned Classification Tree model

### Creating a Classification Tree model instance

```{r}
class_tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 15,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

### Classification Workflow

```{r}
class_tree_wkf <- workflow() |>
  add_recipe(rec) |>
  add_model(class_tree_mod)
```

### Fit Classification Model with tune_grid() and grid_regular()

```{r}
classification_grid <- class_tree_wkf |>
  tune_grid(resamples = data_5_fold,
            grid = grid_regular(tree_depth(), cost_complexity(), levels = 10),
            metrics = metric_set(mn_log_loss)) # Corrected metric
```

### Collecting the metrics computed across the folds for each tuning parameter

```{r}
classification_grid |>
  collect_metrics() |> # collecting defining model metrics
  filter(.metric == "mn_log_loss") # filter by defining model metrics
```

### Using select_best() to pull out best Classification model

```{r}
lowest_log_loss <- classification_grid |>
  select_best(metric = "mn_log_loss")
```

### fit best Classification Tree model to the entire training set to see the model fit

```{r}
classification_final_fit <- class_tree_wkf |>
  finalize_workflow(lowest_log_loss) |>
  last_fit(data_split, metrics = metric_set(mn_log_loss)) # defining model metrics

classification_final_fit |>
  collect_metrics() # collecting defined model metrics
```

### Extracting the model fits for the Classification Tree model

```{r}
# Extract the final model fits
classification_extract_final_model <- extract_workflow(classification_final_fit) 

# View extracted final model fit
classification_extract_final_model
```


## Fitting a tuned Random Forest model

### Creating a Random Forest Instance

```{r}
rf_spec <- rand_forest(mtry = tune()) |>
 set_engine("ranger", importance = "impurity") |>
 set_mode("classification")
```

### Random Forest workflow

```{r}
rf_wkf <- workflow() |>
 add_recipe(rec) |> # Recipe
 add_model(rf_spec) # Defined model instance
```

### CV to Select our Tuning Parameters

```{r}
rf_fit <- rf_wkf |>
 tune_grid(resamples = data_5_fold,
 grid = 10,
 metrics = metric_set(mn_log_loss)) # defining model metrics
```

### Pulling out model metrics

```{r}
rf_fit |>
 collect_metrics() |> # Collecting defined model metrics
 filter(.metric == "mn_log_loss") |>
 arrange(mean) # Arranged by mean RMSE value
```

### Using select_best() to grab the best models tuning parameter values

```{r}
rf_best_params <- rf_fit |>
  select_best(metric = "mn_log_loss") # defining the metric
rf_best_params
```

### Finalizing our model on the training set by fitting this chosen model via finalize_workflow()

```{r}
rf_final_wkf <- rf_wkf |>
 finalize_workflow(rf_best_params)
```

### Fitting the best model to the entire training data set to test on the testing set

```{r}
rf_final_fit <- rf_final_wkf |>
 last_fit(data_split, metrics = metric_set(mn_log_loss)) # defining model metrics
rf_final_fit

rf_final_fit |>
  collect_metrics() # Collecting by defined model metrics
```

### Extracting the final model fit for the Random Forest model

```{r}
# final model fits
rf_final_model <- extract_fit_engine(rf_final_fit) 
rf_final_model
```
## Comparing Best Models

### Best Classification Tree Model

```{r}
classification_final_fit |>
  collect_metrics() # collecting defined model metrics
```

### Best Random Forest Model

```{r}
rf_final_fit |>
  collect_metrics() # Collecting by defined model metrics
```

### Which Model is the Best?

**Best model: Random Forest**

**The Random Forest model has a slightly better log-loss estimate of 0.339 while the Classification Tree model has a log-loss estimate of 0.358 (rounded)**